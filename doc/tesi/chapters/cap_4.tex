% !TEX root=../index.tex

\chapter{Validazione e Testing dei Classificatori}
\label{chap:tuning}
    Al termine della fase di allenamento si ottengono dei classificatori della forma: 
    \begin{equation}\tag{\ref{eq:strong_classifier}}
        F(x) = 
        \begin{cases}
            1 & \text{ se } \sum_{t = 1}^{T} \alpha_t h_t(x) > \theta \sum_{t = 1}^{T} \alpha_t \\
            0 & \text{ altrimenti }
        \end{cases}
    \end{equation}
    La valore di soglia $\theta$, a questo punto, è ancora ignoto e dovrà essere fissato in modo tale da massimizzare le prestazioni del sistema. È necessario, inoltre, impostare il corretto numero di \emph{weak learner} per ogni \emph{strong learner}, al fine di evitare problemi di \emph{overfitting}.
    
    Queste operazioni sono volte alla \emph{validazione} dei vari classificatori e devono essere affiancate da opportune operazioni di \emph{testing} per la valutazione complessiva del sistema.

    \section{Criteri di Valutazione}
    \label{sec:evaluation_criteria}
        Applicando i vari classificator ad un set di elementi si può visualizzare la distribuzione delle istanze degli oggetti rispetto alla classificazione predetta dai classificatori e rispetto a quella reale.

        Si utilizza la \emph{matrice di confusione} per descrivere in modo compatto tale distribuzione.

        \begin{definition}[Matrice di Confusione]
            In un generico problema di classificazione ad $n$ classi per il quale è definito un insieme di \emph{label} $L = \{l_1,..., l_n\}$ ed è stato allenato un classificatore $F(x)$, sia $T = \{(x_1,y_1),..., (x_m, y_m)\}$ un insieme di elementi per cui è nota la reale classificazione.

            Si chiama \emph{matrice di confusione} del classificatore $F$ e relativa all'insieme di elementi $T$, la matrice $C \in \mathbb{N}^{n \times n}$, per cui ogni elemento $c_{ij}$ è pari al numero di oggetti dell'insieme $T$ la cui reale etichettatura corrisponde a $l_i$ e per i quali il classificatore $F(x)$ prevede l'etichettatura $l_j$.
            \begin{equation}
            \label{subeq:confusion_matrix_element}
                 c_{ij} = \#(\{x|(x,l_i) \in T \wedge F(x) = l_j\})
            \end{equation} 
        \end{definition}

        Gli elementi sulla diagonale principale di tale matrice denotano il numero di elementi per cui la predizione fornita dal classificatore è corretta. Infatti, nel caso in cui $i = j$, la relazione \ref{subeq:confusion_matrix_element} diventa:
        \begin{equation}
            c_{ii} = \#(\{x|(x,l_i) \in T \wedge F(x) = l_i\}) \Longrightarrow c_{ii} = \#(\{x|(x,F(x)) \in T\})
        \end{equation}

        Nel caso di classificazione dicotomica, la struttura della matrice di confusione è molto semplice:
        \begin{equation}\label{eq:confusion_matrix}
            C = \left(
            \begin{array}{cc}
            \text{True Positives} & \text{False Negatives}\\
            \text{False Positives} & \text{True Negatives}
            \end{array} \right)
        \end{equation}

        \subsection{Modalità di Test} % (fold)
        \label{sub:dataset_di_validazione}
            % Descrizione delle possibili alternative
            La scelta del set di elementi con i quali testare i classificatori, in fase di valutazione, è particolarmente importante. Esistono alcune alternative:
            \begin{description}
                \item [Training Dataset] Si utilizza lo stesso dataset impiegato per allenare il costruttore. È la soluzione più semplice, ma presenta notevoli svantaggi, in quanto diventa impossibile rilevare ed evitare problemi di \emph{overfitting}.

                \item [Validation Dataset] Si crea un dataset da utilizzare esclusivamente per il testing o per la validazione che non contenga elementi dell'insieme di allenamento.

                \item [Cross Validation] Questa tecnica prevede l'utilizzo di un unico dataset, utilizzato sia per l'allenamento che per la valutazione:
                \begin{enumerate}
                    \item Si suddivide l'insieme in $k$ sottoinsiemi, detti \emph{k-fold}\footnote{Si è notato che, sperimentalmente, sono sufficienti una decina di \emph{fold}.}
                    \item Il classificatore viene allenato utilizzando $k-1$ sottoinsiemi e viene testato utilizzando il sottoinsieme rimanente. Questa operazione viene eseguita $k$ volte.
                    \item La media delle singole prestazioni ottenute nei $k$ esperimenti costituisce le prestazioni complessive del sistema.
                \end{enumerate}

                \item [Split] Si suddivide l'insieme di dati disponibili in due set distinti, uno da utilizzare per l'allenamento, l'altro - solitamente più piccolo del primo - per la validazione e per il testing.
            \end{description}

            % Traiettorie Causali
            Per la validazione e per il testing è stato creato un dataset apposito, estratto da registrazioni completamente diverse da quelle utilizzate per i dataset di allenamento.
            Le porzioni di immagine di profondità che ritraggono la figura della persona, in questo dataset, provengono dalle registrazioni in cui i vari soggetti percorrono delle traiettorie a loro piacimento all'interno dell'area d'interesse.

            % Dimensioni e Negativi
            Il dataset di validazione è costituito da 1500 elementi positivi (immagini che ritraggono persone) e da 1600 elementi negativi.
        % subsection dataset_di_validazione (end)

        \subsection{Misure per Valutare le Prestazioni} % (fold)
        \label{sub:misure_per_valutare_le_prestazioni}
            Scegliere i criteri di valutazione della bontà delle previsioni fornite dai classificatori è un punto cruciale e non banale: l'utilizzo di una misura piuttosto che un'altra dipende dall'obiettivo che si vuole raggiungere.

            Per i problemi di classificazione binaria sono disponibili le seguenti misure per valutarne le prestazioni.
            \begin{description}
                \item [Precision] Porzione di predizioni positive corrette.
                \begin{equation}
                    \label{eq:precision}
                    P = \frac{TP}{TP + FP}
                \end{equation}

                \item [Sensitiviy o Recall] Porzione delle istanze realmente positive classificate correttamente. Per $P$ si intende il numero totale di istanze realmente positive.
                \begin{equation}
                    \label{eq:tpr}
                    TPR = \frac{TP}{P} = \frac{TP}{TP + FN}
                \end{equation}

                \item [Specificity] Porzione di istanze realmente negative classificate correttamente. Per $N$ si intende il numero totale di istanze negative.
                \begin{equation}
                    \label{eq:tnr} 
                    TNR = \frac{TN}{N} = \frac{TN}{TN + FP}
                \end{equation}

                \item [False Positive Rate] Porzione di istanze realmente negative classificate erroneamente come positive.
                \begin{equation}
                    \label{eq:fpr}
                    FPR = \frac{FP}{N} = \frac{FP}{TN + FP} = 1 - TNR
                \end{equation}

                
            \end{description}
        
        % subsection misure_per_valutare_le_prestazioni (end)

    \section{Massimizzazione all'\emph{Accuracy}}
    \label{sec:accuracy_maximization}
        \subsection{Parametri liberi del classificatore}
            \subsubsection{Numero di weak learner}
                Il numero di classificatori deboli per ogni classificatore forte è un parametro libero all'interno della procedura di allenamento.
                Vista la procedura di Adaboost nel capito precedente, potrebbe sembrare che, aggiungendo altri weak learner si possa migliorare progressivamente la precisione del sistema.
                Ciò generalmente non è vero: troppi classificatori deboli potrebbero generare problemi di \emph{overfitting}.
                È necessario quindi conoscere il numero ottimale di classificatori deboli a comporre ciascun classificatore forte.

            \subsubsection{Soglia del classificatore}
                Il valore di soglia relativo a ciascun classificatore forte allenato non è mai stato precisato.
                È necessario quindi impostare tale valore al fine di migliorare complessivamente l'affidabilità del sistema.

        \subsection{Algoritmo di ricerca della soglia e del NWL ottimi}
            I parametri da definire sono quindi il numero di classificatori deboli ed il valore di soglia per ciascuno dei classificatori forti allenati.
            Entrambi vengono scelti, per ogni classificatore forte, al fine di massimizzarne l'accuratezza in relazione al dataset di validazione.
            Massimizzare l'accuratezza implica la massimizzazione dei positivi e dei negativi reali e la conseguente riduzione dei falsi positivi e negativi.

            L'algoritmo per determinare tali valori è fondamentalmente semplice: ogni strong learner viene utilizzato per classificare gli elementi del dataset di validazione.
            L'attività di classificazione, in virtù dei parametri liberi appena identificati, sarà dipendente dalla coppia di valori $(\theta, N_{wl})$ (per $\theta$ si intende il valore di soglia del classificatore e $N_{wl}$ il numero di weak learner che lo compongono).
            Per ogni possibile coppia di parametri, viene misurata l'accuratezza del classificatore nel classificatare correttamente l'intero dataset, quindi viene scelta la coppia per cui l'accuratezza è massima.

            Una procedura di questo tipo, se implementata senza adoperare dei piccoli accorgimenti, potrebbe richiedere molto tempo per essere portata a termine.
            [Algoritmo figo di selezione della coppia minima]

    \section{Analisi dei Risultati} % (fold)
    \label{sec:analisi_dei_risultati}
        Alla termine della procedura di selezione della coppia ottima, si ottengono i valori di accuratezza migliori del sistema.
            % \paragraph{Soglia del classificatore}
            In letteratura il valore di soglia di un classificatore forte si assume essere pari a 0.5 (\cite{Freund97} e \cite{Viola04} aggiungere riferimenti più specifici). Nel lavoro presentato in \cite{Zhu13}, tuttavia, il valore di soglia non viene esplicitato, considerandolo come parametro libero.
            I risultati ottenuti con l'algoritmo di massimizzazione dell'accuracy, in quanto a valore di soglia, sono lievemente differenti da quelli proposti in letteratura, ma non troppo distanti.

            % \paragraph{Numero di classificatori deboli}
            Il numero dei classificatori deboli che compongono ciascun classificatore forte, è molto differente quello ottenuto in altre applicazioni, prima tra tutte il sistema di riconoscimento dei volti di Viola Jones.
            I classificatori forte di questo sistema sono formati da una decina di classificatori deboli, un numero straordinariamente piccolo se confrontato con i migliaia di weak learners necessari a costruire un classificatore di volti.
            Bisogna considerare però la diversa natura dei due problemi e i diversi canali di acquisizione dei dati per il riconoscimento. L'immagine di profondità di una persona ripresa dall'alto non che una grezza rappresentazione dell'individuo se paragonata all'immagine di un volto umano.
            La ricchezza di particolari di un volto, l'incredibile variabilità intraclasse e la sensibilità alle variazioni ambientali (come la luce), rende molto più complesso il problema di riconoscimento e di conseguenza è necessario avere dei classificatori molto più complessi.
            Per raggiungere elevate prestazioni con il framework di Viola-Jones, infatti, è necessario dividere un classificatore forte in diversi stadi, creando dei classificatori a cascata in grado di scartare nel minor tempo possibile, il maggior numero di oggetti che non corrispondono ai volti umani.
            In questo sistema di rilevamento, tutto questo non è necessario. Un numero così basso di classificatori deboli non è dovuto ad un errore, ma è la conseguenza dell'estrema essenzialità delle immagini di profondità HASP.
            Si pensi alle caratteristiche espresse in linguaggio naturale dell'immagine del profilo della persona: sono solamente tre. Quante ne sarebbero necessarie per esprimere le principali caratteristiche di un volto umano?

            % \paragraph{Valore di sensitivity e specificity}
            In base a quanto presente in letteratura, i valori calcolati di \emph{specificity} e \emph{sensitivity} rivelano la bontà del sistema.

    % section analisi_dei_risultati (end)

\chapter{Rilevamento}
\label{chap:rilevamento}
    \section{Tecnica di Rilevamento}
    \label{sec:detection_tecnique}
        \subsection{Detection Window}
            Vengono create delle \emph{detection window} per gestire il rilevamento su frames.
            La detection window viene fatta scorrere sequenzialmente su tutta l'area del frame, in modo da coprirne tutta l'area.
        \subsection{Rilevazione su frame}
            \subsubsection{Resize detection window}
                A causa della variabilità delle dimensioni della figura HASP della persona, si rende necessario poter effettuare dei ridimensionamenti della finestra di rilevamento. 
                Per ridimensionarla bisogna ridimensionare tutte le feature di haar associate ai classificatori della finestra.
            \subsubsection{Slide detection window}
                Si fa scorrere la finestra in modo da coprire tutta l'area del frame.
                Ciò permette di effettuare delle rilevazioni in contesti statici (su singoli frame o su frame non correlati tra di loro).
                Quando si ha a che fare con frame sequenziali, è possibile evitare di analizzare tutte le parti del frame in condizioni di regime, rendendo il processo più veloce.
    \section{Selezione della Finestra Migliore} %@TODO
    \label{sec:best_detection_window}
        \subsection{Introduzione al problema}
        \subsection{Algoritmo di selezione}
    \section{Confronto con l'Algoritmo G-C} %@TODO
    \label{sec:gc_algorithm_comparison}